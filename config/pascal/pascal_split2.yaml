Data:
  data_root: ../data/VOCdevkit2012/VOC2012
  base_data_root: ../data/VOCdevkit2012/VOC2012/base_annotation/
  train_list: ./lists/pascal/voc_sbd_merge_noduplicate.txt
  val_list: ./lists/pascal/val.txt
  classes: 2


Train:
  # svf
  svf: False
  # Aug
  train_h: 480 # 473
  train_w: 480
  val_size: 480
  scale_min: 0.9  # minimum random scale
  scale_max: 1.1 # maximum random scale
  rotate_min: -10  # minimum random rotate
  rotate_max: 10  # maximum random rotate
  ignore_label: 255
  padding_label: 255
  # Dataset & Mode
  split: 2
  shot: 1
  data_set: 'pascal'
  use_split_coco: False # True means FWB setting
  # Optimizer
  batch_size: 4 # batch size for training (bs8 for 1GPU) Transformer:4
  base_lr: 0.0001 # pfenet:0.015 vit-backbone:0.0001 自监督:0.00005
  epochs: 100 # number of epochs to train
  start_epoch: 0
  stop_interval: 60 # stop when the best result is not updated for "stop_interval" epochs
  index_split: -1 # index for determining the params group with 10x learning rate
  power: 0.9 # 0 means no decay
  momentum: 0.9
  weight_decay: 0.0001
  warmup: False
  # Viz & Save & Resume
  print_freq: 10
  save_freq: 10
  resume:  # path to latest checkpoint (default: none, such as epoch_10.pth)  
  # Validate
  evaluate: True
  SubEpoch_val: False # val at the half epoch
  fix_random_seed_val: True
  batch_size_val: 1
  resized_val: True
  ori_resize: True  # use original label for evaluation
  # Else
  workers: 8
  fix_bn: True
  manual_seed: 321
  seed_deterministic: False
  zoom_factor: 8  # zoom factor for final prediction during training, be in [1, 2, 4, 8]
  # Transformer
  backbone: 'DeiT-B/16-384' # vit: ViT-B/16-384 | deit: DeiT-B/16-384
#  backbone: 'ViT-B/16-i21k-384' # vit: ViT-B/16-384 | deit: DeiT-B/16-384
#  backbone: 'ViT-B/16-384' # vit: ViT-B/16-384 | deit: DeiT-B/16-384
  vit_depth: 11 # vit:10 | deit: 11
  vit_stride:
  dataset: 'pascal'
  coco2pascal: False
  bg_num: 5                      # int, number of background proxies
  pt_std: 0.02                   # float, standard deviation of initial prompt tokens (Gaussian)

  



Method:
  aux_weight1: 1.0
  aux_weight2: 1.0
  low_fea: 'layer2'  # low_fea for computing the Gram matrix
  kshot_trans_dim: 2 # K-shot dimensionality reduction
  merge: 'final'     # fusion scheme for GFSS ('base' Eq(S1) | 'final' Eq(18) )
  merge_tau: 0.9     # fusion threshold tau

Test_Finetune:
  weight: best.pth # load weight for fine-tuning or testing (such as train_epoch_107_0.6897.pth)
  ann_type: 'mask' # mask/bbox


#Distributed:
#   dist_url: tcp://127.0.0.1:12345
#   dist_backend: 'nccl'
#   multiprocessing_distributed: False
#   world_size: 2
#   rank: 0
#   use_apex: False
#   opt_level: 'O0'
#   keep_batchnorm_fp32:
#   loss_scale:

